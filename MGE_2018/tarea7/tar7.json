{"paragraphs":[{"text":"%pyspark\n\nimport datetime\na=datetime.datetime.now()\n\nfrom pyspark.sql import SparkSession \nspark = SparkSession.builder.appName('cruise').getOrCreate() \nflights = spark.read.csv('s3n://tarea4/flights.csv',inferSchema=True,header=True)\n\nflights=flights.na.fill(0) #imputamos los missings de las variables numércias\nflights=flights.na.fill(\"?\")#imputamos los missng de las variables categóricas\n\ncolumnList = [item[0] for item in flights.dtypes if item[1].startswith('string')] \n\n#Hacemos la partición de datos 70% para entrenamiento y 30% para test.\ntrain_data,test_data = flights.randomSplit([0.7,0.3])\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.feature import ChiSqSelector\n#Creamos la función indexers para transformar los features de tipo categóricos a numéricos\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(flights) for column in \n            list(set(columnList)) ]\n            \n\nfrom pyspark.ml import Pipeline\n#Creamos un pipeline pindexers para que la función indexers tenga métodos fit transform.\npindexers = Pipeline(stages=indexers) \n\n#Hacemos VectorAssembler para poder generar la variable features que es un vector de los valores de cada columna\n#en este caso eliminamos algunas columnas ya que no hace sentido ocuparlas tipo el año, y variables de horas.\nassembler = VectorAssembler(\n  inputCols=[#'YEAR',Thin\n             'MONTH',\n             'DAY',\n             'DAY_OF_WEEK',\n             #'AIRLINE',\n            'AIRLINE_index',\n             'FLIGHT_NUMBER',\n            # 'TAIL_NUMBER',\n            #'ORIGIN_AIRPORT',\n            'ORIGIN_AIRPORT_index',\n            #'DESTINATION_AIRPORT',\n            'DESTINATION_AIRPORT_index',\n            'SCHEDULED_DEPARTURE',\n            #'DEPARTURE_TIME',\n            'TAXI_OUT',\n            #'WHEELS_OFF',\n            #'SCHEDULED_TIME',\n            'ELAPSED_TIME',\n            'AIR_TIME',\n            'DISTANCE',\n            #'WHEELS_ON',\n            'TAXI_IN',\n            #'SCHEDULED_ARRIVAL',\n            #'ARRIVAL_TIME',\n            'ARRIVAL_DELAY',\n            #'DIVERTED',\n            #'CANCELLED',\n            #'CANCELLATION_REASON',\n            #'AIR_SYSTEM_DELAY',\n            #'SECURITY_DELAY',\n            #'AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY'\n  ],\n    outputCol=\"features\")\n\n\n\n\n#Normalizamos los features\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n\n#aplicamos pca para reducir dimensionalidad\npca = PCA(k=7, inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")\n\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder \nfrom pyspark.ml.evaluation import RegressionEvaluator \nfrom pyspark.ml.regression import LinearRegression,DecisionTreeRegressor,GeneralizedLinearRegression\n\n#Hacemos un udf para generar un tipo diccionario de modelos y de Paramgridbuilders respectivamente y estos\n#se iteren con sus respectivos paraétros.\ndef define_hyper_params():\n\n    #Creamos un diccionario de los modelos\n    modelo = {'dt': DecisionTreeRegressor(featuresCol=\"pcaFeatures\",labelCol='DEPARTURE_DELAY'),\n    'rf': RandomForestRegressor(featuresCol=\"pcaFeatures\",labelCol=\"DEPARTURE_DELAY\")}\n    \n    #Creamos una lista de paramgrids para tener la lista de prarámetros con los que se hará Cross validtion.\n    #en este caso para dt arbol de decision y rf randomforest.\n    search_space = [ParamGridBuilder().\\\n    \n    addGrid(modelo['dt'].maxBins, [10,15,20]).\\\n    addGrid(modelo['dt'].maxDepth, [3,5,10]).\\\n    build()\n    ,\n    ParamGridBuilder().\\\n    addGrid(modelo['rf'].numTrees, [10,15,20]).\\\n    addGrid(modelo['rf'].maxDepth, [5,10,15]).\\\n    build()]\n\n    return (modelo,search_space)\n\n\nfrom pyspark.ml import Pipeline\ndef magic_loop(X_train,models_to_run=['dt','rf']):#entradas dataframe X_train y diccionario de modelos\n    modelo,search_space=define_hyper_params() #usamos la función define_hyper_params para seleccionar \n    #los paramgrids definidos.\n    best=[] #lista para guardar los mejores modelos de cada algoritmo\n    metr=[] #lista para guardar las métricas de ls mejores mdelos de cada algoritmo\n    params=[] #lista para guardar los mejores parámetros de cada algoritmo.\n    for i in range(len(models_to_run)):#corremos para cada modelo sus respectivos parametros\n        #generamos el pipeline de todos los transformers que declaramos\n        pipeline = Pipeline(stages=[pindexers,assembler,scaler,pca,modelo[models_to_run[i]]]) \n        #hacemos el cross validation con la lista de paramétros del models_to_run[i]\n        crossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=search_space[i],#parametros del modelo i\n                          evaluator=RegressionEvaluator(predictionCol='prediction', labelCol=\"DEPARTURE_DELAY\", \n                               metricName='rmse'),#metrica de comparacion default es rmse\n                          numFolds=10)#corremos con 10 partciones el cross validation\n        cvModel = crossval.fit(X_train)#ajuste del modelo\n        best_model=cvModel.bestModel#generamos el mejor modelo con los mejores parametros para el algoritmo i\n        best.append(best_model) #guardamos el mejor modelo del algoritmo i en la lista vest\n        metr.append(min(cvModel.avgMetrics)) #guardamos la metrica rmse en la lista metr del mejor modelo\n        #lo mismo con los parametros\n        params.append(search_space[i][cvModel.avgMetrics.index(min(cvModel.avgMetrics))])\n        \n        #imprimimos los parametros del mejor modelo del algoritmo i\n        print('Mejor modelo de ',models_to_run[i],'fue:',search_space[i][cvModel.avgMetrics.index(min(cvModel.avgMetrics))]) \n    \n    print(metr)\n    \n    #imprimimos el modelo ganador\n    print('Mejor modelo fue:',params[metr.index(min(metr))]) \n    \n    #El mejor modelo es el que tienen el menor rmse y eso regresa el programa.        \n    return(best[metr.index(min(metr))])\n\n\n#guardamos el mejor modelo lo guardamos en champ y tomamos el tiempo de ejecucion de magic loop\nchamp=magic_loop(train_data,models_to_run=['dt','rf'])  \n\nb=datetime.datetime.now()\nprint('tiempo de proceso',b-a)\n\nResults=champ.transform(test_data) \n\nResults.select(\"DEPARTURE_DELAY\",'prediction').show()","user":"anonymous","dateUpdated":"2018-04-21T05:50:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524289774653_-1161845918","id":"20180421-054934_1883481193","dateCreated":"2018-04-21T05:49:34+0000","dateStarted":"2018-04-21T05:50:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:698","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Mejor modelo de ', 'dt', 'fue:', {Param(parent=u'DecisionTreeRegressor_4f58bd55eab8e204a66f', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 20, Param(parent=u'DecisionTreeRegressor_4f58bd55eab8e204a66f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10})\n('Mejor modelo de ', 'rf', 'fue:', {Param(parent=u'RandomForestRegressor_4d60a026d8b77a1f466d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15, Param(parent=u'RandomForestRegressor_4d60a026d8b77a1f466d', name='numTrees', doc='Number of trees to train (>= 1).'): 20})\n[36.208692214061344, 35.77576267436004]\n('Mejor modelo fue:', {Param(parent=u'RandomForestRegressor_4d60a026d8b77a1f466d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15, Param(parent=u'RandomForestRegressor_4d60a026d8b77a1f466d', name='numTrees', doc='Number of trees to train (>= 1).'): 20})\n('tiempo de proceso', datetime.timedelta(0, 25303, 917336))\n+---------------+------------------+\n|DEPARTURE_DELAY|        prediction|\n+---------------+------------------+\n|             -5|12.660428904518506|\n|             -6|15.427485063186078|\n|            289|17.105114160717967|\n|            178|26.388969767381717|\n|             -1|13.241626753270458|\n|             -6| 16.31776000448874|\n|              6| 18.22932922588727|\n|             -6| 9.170978462219505|\n|             39|22.328206851011792|\n|              0|  4.51560266242873|\n|             -4|17.746511373601553|\n|             -1| 7.860441701065662|\n|             -7|25.095373183427068|\n|             -1| 19.03209472215807|\n|              0| 14.53955941852712|\n|             -3|15.404405404580103|\n|             64|19.874781616651042|\n|             -3| 8.370125596589656|\n|             16| 8.128740688829179|\n|              6| 5.822830916489602|\n+---------------+------------------+\nonly showing top 20 rows\n\n"}]},"dateFinished":"2018-04-21T12:53:22+0000"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-04-21T05:50:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524289849049_-732158017","id":"20180421-055049_433253710","dateCreated":"2018-04-21T05:50:49+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:699"}],"name":"tar7","id":"2DCPHYMP6","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}