{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A continuación inicializaremos una sesión de Spark y correremos los queries para responder las preguntas de la vez anterior. Para inicializar Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load('order_details.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employeeid: integer (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- titleofcourtesy: string (nullable = true)\n",
      " |-- birthdate: timestamp (nullable = true)\n",
      " |-- hiredate: timestamp (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- postalcode: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- homephone: string (nullable = true)\n",
      " |-- extension: integer (nullable = true)\n",
      " |-- photo: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- reportsto: integer (nullable = true)\n",
      " |-- photopath: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees  = spark.read.load('employees.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "employees.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: integer (nullable = true)\n",
      " |-- productid: integer (nullable = true)\n",
      " |-- unitprice: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_details = spark.read.load('order_details.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "order_details.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: integer (nullable = true)\n",
      " |-- customerid: string (nullable = true)\n",
      " |-- employeeid: integer (nullable = true)\n",
      " |-- orderdate: timestamp (nullable = true)\n",
      " |-- requireddate: timestamp (nullable = true)\n",
      " |-- shippeddate: timestamp (nullable = true)\n",
      " |-- shipvia: integer (nullable = true)\n",
      " |-- freight: double (nullable = true)\n",
      " |-- shipname: string (nullable = true)\n",
      " |-- shipaddress: string (nullable = true)\n",
      " |-- shipcity: string (nullable = true)\n",
      " |-- shipregion: string (nullable = true)\n",
      " |-- shippostalcode: string (nullable = true)\n",
      " |-- shipcountry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.load('orders.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerid: string (nullable = true)\n",
      " |-- companyname: string (nullable = true)\n",
      " |-- contactname: string (nullable = true)\n",
      " |-- contacttitle: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- postalcode: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- fax: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers = spark.read.load('customers.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: integer (nullable = true)\n",
      " |-- productname: string (nullable = true)\n",
      " |-- supplierid: integer (nullable = true)\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- quantityperunit: string (nullable = true)\n",
      " |-- unitprice: double (nullable = true)\n",
      " |-- unitsinstock: integer (nullable = true)\n",
      " |-- unitsonorder: integer (nullable = true)\n",
      " |-- reorderlevel: integer (nullable = true)\n",
      " |-- discontinued: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.read.load('products.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.load('flights.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read.load('airports.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = spark.read.load('airlines.csv',\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ¿Cuántos \"jefes\" hay en la tabla empleados? ¿Cuáles son estos jefes: número de empleado, nombre, apellido, título, fecha de nacimiento, fecha en que iniciaron en la empresa, ciudad y país?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+---------------+-------------------+-------------------+------+-------+\n",
      "|employeeid|firstname|lastname|titleofcourtesy|          birthdate|           hiredate|  city|country|\n",
      "+----------+---------+--------+---------------+-------------------+-------------------+------+-------+\n",
      "|         2|   Andrew|  Fuller|            Dr.|1952-02-19 00:00:00|1992-08-14 00:00:00|Tacoma|    USA|\n",
      "|         5|   Steven|Buchanan|            Mr.|1955-03-04 00:00:00|1993-10-17 00:00:00|London|     UK|\n",
      "+----------+---------+--------+---------------+-------------------+-------------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.createOrReplaceTempView(\"employees\")\n",
    "p_1 = spark.sql(\"SELECT employeeid,firstname,lastname,titleofcourtesy,birthdate,hiredate,city,country FROM employees where employeeid IN ( select distinct reportsto from employees where reportsto >0)\")\n",
    "p_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=p_1.collect()\n",
    "#p_1.write.save(\"query1.csv\",format=\"csv\")\n",
    "p1s = pd.DataFrame(p1)\n",
    "p1s.to_csv(\"salida1.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. ¿Quién es el segundo \"mejor\" empleado que más órdenes ha generado? (nombre, apellido, título, cuándo entró a la compañía, número de órdenes generadas, número de órdenes generadas por el mejor empleado (número 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+-------------------+-------------+\n",
      "| lastname|firstname|               title|           hiredate|total_ordenes|\n",
      "+---------+---------+--------------------+-------------------+-------------+\n",
      "|  Peacock| Margaret|Sales Representative|1993-05-03 00:00:00|          156|\n",
      "|Leverling|    Janet|Sales Representative|1992-04-01 00:00:00|          127|\n",
      "+---------+---------+--------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.createOrReplaceTempView(\"orders\")\n",
    "p_2 = spark.sql(\"SELECT lastname, firstname, title, hiredate, count(*) as total_ordenes FROM orders JOIN employees ON orders.employeeid = employees.employeeid GROUP BY lastname, firstname, title, hiredate ORDER BY total_ordenes DESC LIMIT 2\")\n",
    "p_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2=p_2.collect()\n",
    "#p_2.write.save(\"query2.csv\",format=\"csv\")\n",
    "p2s = pd.DataFrame(p2)\n",
    "p2s.to_csv(\"salida2.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### c. ¿Cuál es el delta de tiempo más grande entre una orden y otra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|orderid|          orderdate|delta|\n",
      "+-------+-------------------+-----+\n",
      "|  10262|1996-07-22 00:00:00|    3|\n",
      "|  10284|1996-08-19 00:00:00|    3|\n",
      "|  10267|1996-07-29 00:00:00|    3|\n",
      "|  10256|1996-07-15 00:00:00|    3|\n",
      "|  10273|1996-08-05 00:00:00|    3|\n",
      "+-------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_3=spark.sql(\"SELECT orderid, from_utc_timestamp(date_format(orderdate,'yyyy-MM-dd HH:mm:ss.SSS'),'UTC') orderdate, datediff(orderdate,from_utc_timestamp(date_format(lag(orderdate) OVER(ORDER BY orderid),'yyyy-MM-dd HH:mm:ss.SSS'),'UTC')) delta FROM orders ORDER BY delta DESC LIMIT 5\")\n",
    "p_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3=p_3.collect()\n",
    "#p_3.write.save(\"query3.csv\",format=\"csv\")\n",
    "p3s = pd.DataFrame(p3)\n",
    "p3s.to_csv(\"salida3.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ¿Qué aerolíneas (nombres) llegan al aeropuerto \"Honolulu International Airport\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select distinct airlines.airline from flights join airlines on flights1.airline = airlines.iata_code where flights.destination_airport = \"HNL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.createOrReplaceTempView(\"airlines\")\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             airline|\n",
      "+--------------------+\n",
      "|      Virgin America|\n",
      "|United Air Lines ...|\n",
      "|     US Airways Inc.|\n",
      "|Hawaiian Airlines...|\n",
      "|Alaska Airlines Inc.|\n",
      "|Delta Air Lines Inc.|\n",
      "|American Airlines...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_4 = spark.sql(\"select distinct airlines.airline from flights join airlines on flights.airline = airlines.iata_code where flights.destination_airport = 'HNL'\")\n",
    "p_4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4=p_4.collect()\n",
    "#p_4.write.save(\"query4.csv\",format=\"csv\")\n",
    "import pandas as pd\n",
    "p4s = pd.DataFrame(p4)\n",
    "p4s.to_csv(\"salida4.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### b. ¿En qué horario (hora del día, no importan los minutos) hay salidas del aeropuerto de San Francisco (\"SFO\") a \"Honolulu International Airport\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|horario|\n",
      "+-------+\n",
      "|     15|\n",
      "|     11|\n",
      "|     73|\n",
      "|     85|\n",
      "|     16|\n",
      "|     18|\n",
      "|     70|\n",
      "|     17|\n",
      "|     90|\n",
      "|     19|\n",
      "|     93|\n",
      "|     95|\n",
      "|     84|\n",
      "|     10|\n",
      "|     65|\n",
      "|     12|\n",
      "|     83|\n",
      "|     13|\n",
      "|     14|\n",
      "|     94|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_5 = spark.sql(\"select distinct SUBSTRING( scheduled_departure,1,2) as horario from flights where origin_airport == 'SFO' and destination_airport == 'HNL'\")\n",
    "p_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5=p_5.collect()\n",
    "#p_5.write.save(\"query5.csv\",format=\"csv\")\n",
    "p5s = pd.DataFrame(p5)\n",
    "p5s.to_csv(\"salida5.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. ¿Qué día de la semana y en qué aerolínea nos conviene viajar a \"Honolulu International Airport\" para tener el menor retraso posible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+\n",
      "|day_of_week|             airline|retraso|\n",
      "+-----------+--------------------+-------+\n",
      "|          5|Hawaiian Airlines...|    -27|\n",
      "+-----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_6 = spark.sql(\"select day_of_week, airlines.airline, MIN(departure_delay) AS retraso from flights join airlines on flights.airline = airlines.iata_code where destination_airport == 'HNL' group by day_of_week, airlines.airline order by retraso asc LIMIT 1\")\n",
    "p_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6=p_6.collect()\n",
    "#p_6.write.save(\"query6.csv\",format=\"csv\")\n",
    "p6s = pd.DataFrame(p6)\n",
    "p6s.to_csv(\"salida6.csv\", sep=',',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. ¿Cuál es el aeropuerto con mayor tráfico de entrada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_7= spark.sql(\"select DESTINATION_AIRPORT, airlines.airline, count(*) as trafico from flights join airlines on flights.airline = airlines.iata_code group by DESTINATION_AIRPORT, airlines.airline order by trafico desc limit 1\")\n",
    "p_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7=p_7.collect()\n",
    "#p_7.write.save(\"query7.csv\",format=\"csv\")\n",
    "p7s = pd.DataFrame(p7)\n",
    "p7s.to_csv(\"salida7.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. ¿Cuál es la aerolínea con mayor retraso de salida por día de la semana?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_8= spark.sql(\"select f.day_of_week, f.airline, f.departure_delay from(select day_of_week, max(departure_delay) as retraso from flights group by day_of_week) as x inner join flights as f on f.day_of_week = x.day_of_week and f.departure_delay = x.retraso\")\n",
    "p_8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "p8=p_8.collect()\n",
    "#p_8.write.save(\"query8.csv\",format=\"csv\")\n",
    "p8s = pd.DataFrame(p8)\n",
    "p8s.to_csv(\"salida8.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. ¿Cuál es la tercer aerolínea con menor retraso de salida los lunes (day of week = 2)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_9=spark.sql(\"select airlines.airline, min(departure_delay) AS retraso from flights join airlines on flights.airline = airlines.iata_code where day_of_week == 2 group by airlines.airline order by retraso asc limit 2\")\n",
    "p_9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "p9=p_9.collect()\n",
    "#p_9.write.save(\"query9.csv\",format=\"csv\")\n",
    "p9s = pd.DataFrame(p9)\n",
    "p9s.to_csv(\"salida9.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. ¿Cuál es el aeropuerto origen que llega a la mayor cantidad de aeropuertos destino diferentes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select origin_airport, count(distinct destination_airport) as total from flights group by origin_airport order by total desc limit 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|origin_airport|total|\n",
      "+--------------+-----+\n",
      "|           ATL|  169|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_10 = spark.sql(\"select origin_airport, count(distinct destination_airport) as total from flights group by origin_airport order by total desc limit 1\")\n",
    "p_10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10=p_10.collect()\n",
    "#p_9.write.save(\"query9.csv\",format=\"csv\")\n",
    "p10s = pd.DataFrame(p10)\n",
    "p10s.to_csv(\"salida10.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
