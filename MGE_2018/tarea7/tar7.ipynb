{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init('/home/abraham/spark-2.2.1-bin-hadoop2.7') \n",
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc) \n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName('cruise').getOrCreate() \n",
    "flights = spark.read.csv('/home/abraham/MGE_2018/tarea4/flights.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, AIRLINE: string, FLIGHT_NUMBER: int, TAIL_NUMBER: string, ORIGIN_AIRPORT: string, DESTINATION_AIRPORT: string, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, TAXI_OUT: int, WHEELS_OFF: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, AIR_TIME: int, DISTANCE: int, WHEELS_ON: int, TAXI_IN: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, CANCELLATION_REASON: string, AIR_SYSTEM_DELAY: int, SECURITY_DELAY: int, AIRLINE_DELAY: int, LATE_AIRCRAFT_DELAY: int, WEATHER_DELAY: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flights=flights.na.fill(0) #imputamos los missings de las variables numércias\n",
    "flights=flights.na.fill(\"?\")#imputamos los missng de las variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seleccionamos la lista de variables de tipo string y las guardamos en la variable columnList\n",
    "columnList = [item[0] for item in flights.dtypes if item[1].startswith('string')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hacemos la partición de datos 70% para entrenamiento y 30 para test.\n",
    "train_data,test_data = flights.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "#Creamos la función indexers para transformar los features de tipo categóricos a numéricos\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(flights) for column in \n",
    "            list(set(columnList)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#Creamos un pipeline pindexers para que la función indexers tenga métodos fit transform.\n",
    "pindexers = Pipeline(stages=indexers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hacemos VectorAssembler para poder generar la variable features que es un vector de los valores de cada columna\n",
    "#en este caso eliminamos algunas columnas ya que no hace sentido ocuparlas tipo el año, y variables de horas.\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=[#'YEAR',Thin\n",
    "             'MONTH',\n",
    "             'DAY',\n",
    "             'DAY_OF_WEEK',\n",
    "             #'AIRLINE',\n",
    "            'AIRLINE_index',\n",
    "             'FLIGHT_NUMBER',\n",
    "            # 'TAIL_NUMBER',\n",
    "            #'ORIGIN_AIRPORT',\n",
    "            'ORIGIN_AIRPORT_index',\n",
    "            #'DESTINATION_AIRPORT',\n",
    "            'DESTINATION_AIRPORT_index',\n",
    "            'SCHEDULED_DEPARTURE',\n",
    "            #'DEPARTURE_TIME',\n",
    "            'TAXI_OUT',\n",
    "            #'WHEELS_OFF',\n",
    "            #'SCHEDULED_TIME',\n",
    "            'ELAPSED_TIME',\n",
    "            'AIR_TIME',\n",
    "            'DISTANCE',\n",
    "            #'WHEELS_ON',\n",
    "            'TAXI_IN',\n",
    "            #'SCHEDULED_ARRIVAL',\n",
    "            #'ARRIVAL_TIME',\n",
    "            'ARRIVAL_DELAY',\n",
    "            #'DIVERTED',\n",
    "            #'CANCELLED',\n",
    "            #'CANCELLATION_REASON',\n",
    "            #'AIR_SYSTEM_DELAY',\n",
    "            #'SECURITY_DELAY',\n",
    "            #'AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY'\n",
    "  ],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalizamos los features\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aplicamos pca para reducir dimensionalidad\n",
    "pca = PCA(k=7, inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hacemos un udf para generar un tipo diccionario de modelos y de Paramgridbuilders rspectivamente y estos\n",
    "#se iteren con sus respectivos paraétros.\n",
    "def define_hyper_params():\n",
    "\n",
    "    #Creamos un diccionario de los modelos\n",
    "    modelo = {'lr': LinearRegression(featuresCol=\"pcaFeatures\",labelCol='DEPARTURE_DELAY'),\n",
    "    'rf': RandomForestRegressor(featuresCol=\"pcaFeatures\",labelCol=\"DEPARTURE_DELAY\")}\n",
    "    \n",
    "    #Creamos una lista de paramgrids para tener la lista de prarámetros con los que se hará Cross validtion.\n",
    "    #en este caso para lr y rf.\n",
    "    search_space = [ParamGridBuilder().\\\n",
    "    addGrid(modelo['lr'].regParam, [0,0.01,0.05,0.1,.2]).\\\n",
    "    addGrid(modelo['lr'].elasticNetParam, [0,.01,.05,.1,.2]).\\\n",
    "    build()\n",
    "    ,\n",
    "    ParamGridBuilder().\\\n",
    "    addGrid(modelo['rf'].numTrees, [10,20]).\\\n",
    "    addGrid(modelo['rf'].maxDepth, [5, 10]).\\\n",
    "    build()]\n",
    "\n",
    "    return (modelo,search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "def magic_loop(X_train,models_to_run=['lr','rf']):#entradas dataframe X_train y diccionario de modelos\n",
    "    modelo,search_space=define_hyper_params() #usamos la función define_hyper_params para seleccionar \n",
    "    #los paramgrids definidos.\n",
    "    best=[] #lista para guardar los mejores modelos de cada algoritmo\n",
    "    metr=[] #lista para guardar las métricas de ls mejores mdelos de cada algoritmo\n",
    "    params=[] #lista para guardar los mejores parámetros de cada algoritmo.\n",
    "    for i in range(len(models_to_run)):#corremos para cada modelo sus respectivos parametros\n",
    "        #generamos el pipeline de todos los transformers que declaramos\n",
    "        pipeline = Pipeline(stages=[pindexers,assembler,scaler,pca,modelo[models_to_run[i]]]) \n",
    "        #hacemos el cross validation con la lista de paramétros del models_to_run[i]\n",
    "        crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=search_space[i],#parametros del modelo i\n",
    "                          evaluator=RegressionEvaluator(predictionCol='prediction', labelCol=\"DEPARTURE_DELAY\", \n",
    "                               metricName='rmse'),#metrica de comparacion default es rmse\n",
    "                          numFolds=2)#corremos con 10 partciones el cross validation\n",
    "        cvModel = crossval.fit(X_train)#ajuste del modelo\n",
    "        best_model=cvModel.bestModel#generamos el mejor modelo con los mejores parametros para el algoritmo i\n",
    "        best.append(best_model) #guardamos el mejor modelo del algoritmo i en la lista vest\n",
    "        metr.append(min(cvModel.avgMetrics)) #guardamos la metrica rmse en la lista metr del mejor modelo\n",
    "        #lo mismo con los parametros\n",
    "        params.append(search_space[i][cvModel.avgMetrics.index(min(cvModel.avgMetrics))])\n",
    "        \n",
    "        #imprimimos los parametros del mejor modelo del algoritmo i\n",
    "        print('Mejor modelo de ',models_to_run[i],'fue:',search_space[i][cvModel.avgMetrics.index(min(cvModel.avgMetrics))]) \n",
    "    \n",
    "    print(metr)\n",
    "    \n",
    "    #imprimimos el modelo ganador\n",
    "    print('Mejor modelo fue:',params[metr.index(min(metr))]) \n",
    "    \n",
    "    #El mejor modelo es el que tienen el menor rmse y eso regresa el programa.        \n",
    "    return(best[metr.index(min(metr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de  lr fue: {Param(parent='LinearRegression_481b82250acf6fe8d39e', name='regParam', doc='regularization parameter (>= 0).'): 0, Param(parent='LinearRegression_481b82250acf6fe8d39e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0}\n",
      "Mejor modelo de  rf fue: {Param(parent='RandomForestRegressor_45b5a0b3dff6e079312f', name='numTrees', doc='Number of trees to train (>= 1).'): 20}\n",
      "[36.51568244674939, 36.24954170922881]\n",
      "Mejor modelo fue: {Param(parent='RandomForestRegressor_45b5a0b3dff6e079312f', name='numTrees', doc='Number of trees to train (>= 1).'): 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_48bea2dd173a8762eefc"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "magic_loop(train_data,models_to_run=['lr','rf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de  lr fue: {Param(parent='LinearRegression_48ff93cf0a23b4c74f5b', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0, Param(parent='LinearRegression_48ff93cf0a23b4c74f5b', name='regParam', doc='regularization parameter (>= 0).'): 0.01}\n",
      "Mejor modelo de  rf fue: {Param(parent='RandomForestRegressor_4c46965d7d6e12ef7014', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestRegressor_4c46965d7d6e12ef7014', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10}\n",
      "[36.498729960099396, 35.99097917344129]\n",
      "Mejor modelo fue: {Param(parent='RandomForestRegressor_4c46965d7d6e12ef7014', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestRegressor_4c46965d7d6e12ef7014', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10}\n",
      "CPU times: user 2.51 s, sys: 761 ms, total: 3.27 s\n",
      "Wall time: 1h 31min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#guardamos el mejor modelo lo guardamos en champ y tomamos el tiempo de ejecucion de magic loop\n",
    "champ=magic_loop(train_data,models_to_run=['lr','rf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073924"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ajustamos el modelo ganador a los datos de test.\n",
    "Results=champ.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|DEPARTURE_DELAY|        prediction|\n",
      "+---------------+------------------+\n",
      "|            108| 18.64875623400596|\n",
      "|            -11|  6.28328632994873|\n",
      "|             -6|18.552537589269697|\n",
      "|             -4|3.1351813280716385|\n",
      "|              3|19.105606553924364|\n",
      "|             -6|6.2470002388006005|\n",
      "|             -3|17.141745772777128|\n",
      "|             -4|19.105606553924364|\n",
      "|             23| 7.492240299219928|\n",
      "|              9|18.896973779743256|\n",
      "|             -7| 7.055545795550583|\n",
      "|              3|18.900372410219646|\n",
      "|              0| 7.799763384514726|\n",
      "|             -5| 8.110033905954362|\n",
      "|             -2| 7.638137819692622|\n",
      "|            121| 19.26445126012523|\n",
      "|              4|4.9338790903378955|\n",
      "|              5|18.552537589269697|\n",
      "|             16|12.296915427067606|\n",
      "|              1|18.776194097934486|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Results.select(\"DEPARTURE_DELAY\",'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo de proceso 0:00:00.000144\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "a=datetime.datetime.now()\n",
    "\n",
    "b=datetime.datetime.now()\n",
    "print('tiempo de proceso',b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
