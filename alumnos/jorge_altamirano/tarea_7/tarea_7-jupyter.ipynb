{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 7\n",
    "\n",
    "_175904 - Jorge III Altamirano Astorga_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importo librerías\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrameStatFunctions, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import re as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arranque de Spark\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.cores\", 4)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"32g\")\n",
    "conf.set(\"spark.executor.cores\", 12)\n",
    "conf.set(\"spark.jars\", \"/home/jaa6766\")\n",
    "sc = SparkContext(master = \"local[14]\", sparkHome=\"/usr/local/spark/\", \n",
    "                  appName=\"tarea-mge-7\", conf=conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                SEA|                  5|          2354|            -11|      21|        15|           205|         194|     169|    1448|      404|      4|              430|         408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                PBI|                 10|             2|             -8|      12|        14|           280|         279|     263|    2330|      737|      4|              750|         741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#carga de datos\n",
    "flights = spark.read.csv(\"hdfs://172.17.0.2:9000/data/flights/flights.csv\", \n",
    "                         inferSchema=True, \n",
    "                         header=True)\n",
    "flights = flights.cache()\n",
    "flights.show(2)\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'SCHEDULED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'CANCELLED',\n",
       " 'DEPARTURE_DELAY']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columnas más relevantes\n",
    "relevant_cols = [\"YEAR\", \"MONTH\", \"DAY\", \"DAY_OF_WEEK\", \"AIRLINE\", \n",
    "                \"FLIGHT_NUMBER\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\",\n",
    "                \"SCHEDULED_DEPARTURE\", \"DEPARTURE_TIME\", \"SCHEDULED_TIME\",\n",
    "                \"AIR_TIME\", \"DISTANCE\", \"SCHEDULED_ARRIVAL\", \"CANCELLED\",\n",
    "                \"DEPARTURE_DELAY\"]\n",
    "relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+--------------+-------------------+-------------------+--------------+--------------+--------+--------+-----------------+---------+---------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|SCHEDULED_TIME|AIR_TIME|DISTANCE|SCHEDULED_ARRIVAL|CANCELLED|DEPARTURE_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+--------------+-------------------+-------------------+--------------+--------------+--------+--------+-----------------+---------+---------------+\n",
      "|2015|    1|  1|          4|     AS|           98|           ANC|                SEA|                  5|          2354|           205|     169|    1448|              430|        0|            -11|\n",
      "|2015|    1|  1|          4|     AA|         2336|           LAX|                PBI|                 10|             2|           280|     263|    2330|              750|        0|             -8|\n",
      "+----+-----+---+-----------+-------+-------------+--------------+-------------------+-------------------+--------------+--------------+--------+--------+-----------------+---------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### selección de columnas\n",
    "flights = flights.select(relevant_cols)\n",
    "### descartar nulos\n",
    "flights = flights.na.drop().cache()\n",
    "### dividir en sets de entrenamiento y set de pruebas\n",
    "flights.write.parquet(\"hdfs://172.17.0.2:9000/tmp/flights-tmp.parquet\", mode=\"overwrite\")\n",
    "flights.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear sets de Entrenamiento y Pruebas\n",
    "\n",
    "Aquí también se tienen los objetos que se utilizarán en el pipeline más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features que se tomarán para la predicción:  ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'FLIGHT_NUMBER', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'SCHEDULED_TIME', 'AIR_TIME', 'DISTANCE', 'SCHEDULED_ARRIVAL', 'CANCELLED', 'airline_indexer', 'origin_indexer', 'destination_indexer']\n",
      "CPU times: user 59.8 ms, sys: 17 ms, total: 76.7 ms\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#por eficiencia he visto que es mejor guardarlo\n",
    "flights = spark.read.parquet(\"hdfs://172.17.0.2:9000/tmp/flights-tmp.parquet\")\n",
    "flights = flights.cache()\n",
    "#dividir el set\n",
    "(train, test) = flights.randomSplit([0.7, 0.3], seed=175904)\n",
    "train2 = train.sample(fraction=0.001, seed=175904)\n",
    "\n",
    "#guardamos por eficiencia, nuevamente\n",
    "train.write.parquet(\"hdfs://172.17.0.2:9000/tmp/train-tmp.parquet\", mode=\"overwrite\")\n",
    "test.write.parquet(\"hdfs://172.17.0.2:9000/tmp/test-tmp.parquet\", mode=\"overwrite\")\n",
    "train = spark.read.parquet(\"hdfs://172.17.0.2:9000/tmp/train-tmp.parquet\")\n",
    "test = spark.read.parquet(\"hdfs://172.17.0.2:9000/tmp/test-tmp.parquet\")\n",
    "train = train.cache()\n",
    "test = test.cache()\n",
    "\n",
    "#descartando categóricas y la que vamos a predecir: \"DEPARTURE_DELAY\"\n",
    "features = [ x for x in train.schema.names if x not in \n",
    "            [ \"DEPARTURE_DELAY\", \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"] ]\n",
    "#agregando las columnas que ya van a ser indizadas\n",
    "features.append(\"airline_indexer\")\n",
    "features.append(\"origin_indexer\")\n",
    "features.append(\"destination_indexer\")\n",
    "print(\"Features que se tomarán para la predicción: \", features)\n",
    "\n",
    "#indización de columnas: de categóricas a texto\n",
    "li0 = StringIndexer(inputCol=\"AIRLINE\",\n",
    "                    outputCol=\"airline_indexer\", \n",
    "                    handleInvalid=\"skip\") \\\n",
    "    .fit(flights)\n",
    "li1 = StringIndexer(inputCol=\"ORIGIN_AIRPORT\",\n",
    "                    outputCol=\"origin_indexer\", \n",
    "                    handleInvalid=\"skip\") \\\n",
    "    .fit(flights)\n",
    "li2 = StringIndexer(inputCol=\"DESTINATION_AIRPORT\",\n",
    "                    outputCol=\"destination_indexer\", \n",
    "                    handleInvalid=\"skip\") \\\n",
    "    .fit(flights)\n",
    "li  = StringIndexer(inputCol=\"DEPARTURE_DELAY\",\n",
    "                    outputCol=\"delay_indexer\", \n",
    "                    handleInvalid=\"skip\") \\\n",
    "    .fit(flights)\n",
    "#este ensamble es requerido para usar los clasificadores y regresores de Spark ML\n",
    "va0 = VectorAssembler() \\\n",
    "    .setInputCols(features) \\\n",
    "    .setOutputCol(\"features\")\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"features_pca\")\n",
    "lc = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                   labels=li.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas y componentes ML de los pipelines\n",
    "\n",
    "Esto no va a ir en el documento final, dado que sólo estuve _jugando_. Esto resultó necesario para entender mejor cómo funciona y calcular tiempos.\n",
    "\n",
    "Es destacable que estos son objetos que se utilizarán en el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 ms, sys: 1.3 ms, total: 10.3 ms\n",
      "Wall time: 24.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# regresión logística\n",
    "lr = LogisticRegression(featuresCol=\"features_pca\",\n",
    "                        labelCol=\"delay_indexer\",\n",
    "                        #maxIter=10, elasticNetParam=0.8,\n",
    "                        regParam=0.3, \n",
    "                        family=\"multinomial\",\n",
    "                        predictionCol=\"prediction\")\n",
    "#creación del pipeline, este no se va a utilizar en el Magic Loop\n",
    "pipeline1 = Pipeline(stages=[li0, li1, li2, va0, pca, li, lr, lc]) \n",
    "# parámetros que se van a utilizar en regresión logística\n",
    "pgLR = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.5, 0.8]) \\\n",
    "    .addGrid(lr.maxIter, [2,5,10]) \\\n",
    "    .build()\n",
    "\n",
    "#el número de folds es inválido, incluso no recomendado directamente en la documentación de \n",
    "#Spark, sin embargo, es un punto de inicio. \n",
    "#\n",
    "#Este componente no se va a utilizar en el Magic Loop\n",
    "cv1 = CrossValidator(estimator=pipeline1,\n",
    "                      estimatorParamMaps=pgLR,\n",
    "                      evaluator=MulticlassClassificationEvaluator(labelCol=\"DEPARTURE_DELAY\"),\n",
    "                      numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms\n",
      "Wall time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# regresión lineal\n",
    "glr = GeneralizedLinearRegression(featuresCol=\"features_pca\",\n",
    "                        labelCol=\"delay_indexer\", family=\"Gaussian\")\n",
    "# parámetros que se van a utilizar para regresión lineal\n",
    "pgGLR = ParamGridBuilder() \\\n",
    "    .addGrid(glr.family, [\"Gaussian\", \"Poisson\", \"Tweedie\"]) \\\n",
    "    .addGrid(glr.maxIter, [10, 25, 50]) \\\n",
    "    .build()\n",
    "#creación del pipeline, este no se va a utilizar en el Magic Loop\n",
    "pipeline2 = Pipeline(stages=[li0, li1, li2, va0, pca, li, glr, lc])\n",
    "#el número de folds es inválido, incluso no recomendado directamente en la documentación de \n",
    "#Spark, sin embargo, es un punto de inicio. \n",
    "#\n",
    "#Este componente no se va a utilizar en el Magic Loop\n",
    "cv2 = CrossValidator(estimator=pipeline2,\n",
    "                          estimatorParamMaps=pgGLR,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"DEPARTURE_DELAY\"),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Loop\n",
    "\n",
    "Aquí definí la función de Magic Loop. Le llamé magic_loop3 dado que es la función que nombramos y utilizamos el semestre pasado Augusto Sagón era magic_loop2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación del Magic Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fit on prepipe...\n",
      "Starting transform on prepipe...\n",
      "Starting fit on LogisticRegression...\n",
      "Starting transform on LogisticRegression...\n",
      "Starting error calculation on LogisticRegression...\n",
      "Error f1: 0.006251\n",
      "LogisticRegression is the best model so far: 0.006251 (f1)\n",
      "Starting fit on prepipe...\n",
      "Starting transform on prepipe...\n",
      "Starting fit on GeneralizedLinearRegression...\n",
      "Starting transform on GeneralizedLinearRegression...\n",
      "Starting error calculation on GeneralizedLinearRegression...\n",
      "Error f1: 0.000000\n",
      "CPU times: user 12min 16s, sys: 4min 22s, total: 16min 38s\n",
      "Wall time: 13h 53min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DEBUG = False\n",
    "def magic_loop3(pipelines, grid, train, test, cvfolds=3):\n",
    "    best_score = 0.0 #symbolic high value :-)\n",
    "    best_grid = None #inicializar la variable\n",
    "    #este loop inicia las pruebas secuenciales de los pipelines:\n",
    "    #es relevante que no sólo soporta 2, sino se va en cada uno \n",
    "    #de los que estén presentes en la lista\n",
    "    for pipe in pipelines:\n",
    "        try:\n",
    "            #quiero que se desligue, para no modificar (al final, usa poco RAM)\n",
    "            pipe = pipe.copy()\n",
    "            #etapas del pipeline\n",
    "            stages = pipe.getStages()\n",
    "            #obtener el predictor (el motor ML)\n",
    "            predictor = [stage for stage in stages\n",
    "                if \"pyspark.ml.classification\" in str(type(stage)) or\n",
    "                 \"pyspark.ml.regression\" in str(type(stage))][0]\n",
    "            predictor_i = stages.index(predictor)\n",
    "            stringer = [stage for stage in pipe.getStages()\n",
    "                if \"pyspark.ml.feature.StringIndexer\" in str(type(stage))][0]\n",
    "            if DEBUG: print(\"pipeline:\\n%s\\n\\n\"%stages)\n",
    "            if DEBUG: print(\"predictor=%s (index %s, type %s), stringer=%s (%s)\\n\"%\n",
    "                  (predictor, stages.index(predictor), type(predictor),\n",
    "                  stringer, type(stringer)))\n",
    "            #dado que no son predicciones susceptibles a cambios en el CV\n",
    "            prepipe = Pipeline(stages=stages[0:(predictor_i)])\n",
    "            if DEBUG: print(\"pre pipeline:\\n%s\\n\\n\"%prepipe.getStages())\n",
    "            print(\"Starting fit on prepipe...\")\n",
    "            #modelo para el prepipeline\n",
    "            prepipem = prepipe.fit(train)\n",
    "            print(\"Starting transform on prepipe...\")\n",
    "            train2 = prepipem.transform(train)\n",
    "            test2 = prepipem.transform(test)\n",
    "            #el motor ML sí es susceptible CV\n",
    "            postpipe = Pipeline(stages=stages[(predictor_i):len(stages)])\n",
    "            if DEBUG: print(\"post pipeline:\\n%s\\n\\n\"%postpipe.getStages())\n",
    "            #creación del Cross Validator\n",
    "            gridcv = CrossValidator(estimator=postpipe,\n",
    "                          estimatorParamMaps=grid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"DEPARTURE_DELAY\"),\n",
    "                          numFolds=cvfolds)\n",
    "            #extraemos el nombre y le quitamos la parte \"fea\" que devuelve type()\n",
    "            predictr = [str(type(stage)) for stage in pipe.getStages() \n",
    "            if \"pyspark.ml.classification\" in str(type(stage)) or\n",
    "             \"pyspark.ml.regression\" in str(type(stage))][0]\n",
    "            predictr = re.sub(\"^<class 'pyspark\\.ml\\.(classification|regression)\\.([a-zA-Z0-9]+)'>\", \n",
    "                              \"\\\\2\", \n",
    "                              predictr)\n",
    "            #creación del modelo\n",
    "            print(\"Starting fit on %s...\"%predictr)\n",
    "            gridcvm = gridcv.fit(train2)\n",
    "            #aplicación del modelo\n",
    "            print(\"Starting transform on %s...\"%predictr)\n",
    "            preds = gridcvm.transform(test2)\n",
    "            #obtenemos el evaluador para el uso en la medición de errores\n",
    "            ev = gridcvm.getEvaluator()\n",
    "            #obtenemos la métrica del error\n",
    "            metric = ev.getMetricName()\n",
    "            print(\"Starting error calculation on %s...\"%predictr)\n",
    "            #obtenemos el valor del eror\n",
    "            error  = ev.evaluate(preds)\n",
    "            print(\"Error %s: %f\"% (metric, error))\n",
    "            #si es mejor que el modelo pasado, lo guardamos. El último\n",
    "            #guardado será el que devuelva esta función\n",
    "            if error > best_score:\n",
    "                print(\"%s is the best model so far: %f (%s)\"%(predictr, error, metric))\n",
    "                best_grid = gridcvm\n",
    "        #manejo de errores y horrores\n",
    "        except Exception as e:\n",
    "            print('Error during Magic Loop:', e)\n",
    "        continue\n",
    "    return best_grid\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(glr.family, [\"Gaussian\", \"Poisson\", \"Tweedie\"]) \\\n",
    "                .addGrid(glr.maxIter, [1, 2, 3]) \\\n",
    "                .addGrid(lr.maxIter, [1, 2, 3]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.1,0.2,0.3]) \\\n",
    "                .build()\n",
    "magic = magic_loop3(pipelines, paramGrid, train, test, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor Modelo\n",
    "\n",
    "Pruebas con el mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression (best model) Parameters: maxIter=100, elasticNetParam=0.000000\n",
      "Error f1: 0.006251\n",
      "CPU times: user 393 ms, sys: 131 ms, total: 524 ms\n",
      "Wall time: 9.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#obtengo el pipeline que devolvió el magic loop\n",
    "best_model = magic.getEstimator()\n",
    "#obtenemos el paso del clasificador\n",
    "best_estimator = best_model.getStages()[0]\n",
    "#guardo en una variable los parámetros más adecuados\n",
    "best_estimator_params = best_estimator.extractParamMap()\n",
    "#obtener el evaluador para medir errores\n",
    "ev0 = magic.getEvaluator()\n",
    "#impresión de los parámetros y el mejor modelo\n",
    "print(\"%s (best model) Parameters: maxIter=%d, elasticNetParam=%f\"%\n",
    "      (re.sub(\"^<class 'pyspark\\.ml\\.(classification|regression)\\.([a-zA-Z0-9]+)'>\", \n",
    "                              \"\\\\2\", (str)((type(best_estimator)))), \n",
    "       best_estimator_params[best_estimator.maxIter], \n",
    "       best_estimator_params[best_estimator.elasticNetParam]))\n",
    "#aquí probé como se veían los errores\n",
    "preds = magic.transform(Pipeline(stages=pipeline1.getStages()[0:6]).fit(test).transform(test))\n",
    "print(\"Error %s: %f\"% (ev0.getMetricName(), ev0.evaluate(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras pruebas\n",
    "\n",
    "El primer modelo que probé fue el de Bosques aleatorios. Esto fue sin éxito: además de que consumía mucho tiempo tenía un error muy similar a Regresión Logística. Por eso tiene un número de pipeline 0: inicié como se debe, con 0 la numeración de mis objetos. :-)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<pre>\n",
    "%%time\n",
    "#random forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features_pca\",\n",
    "                        labelCol=\"delay_indexer\",\n",
    "                        #maxDepth=10, numTrees=4,\n",
    "                        maxBins=629, seed=175904, \n",
    "                        #maxIter=10, regParam=0.01, \n",
    "                        predictionCol=\"prediction\")\n",
    "# parámetros que se van a utilizar para random forest\n",
    "pgRF = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(rf.numTrees, [4, 3, 4]) \\\n",
    "    .build()\n",
    "# creación del pipeline\n",
    "pipeline0 = Pipeline(stages=[li0, li1, li2, va0, pca, li, rf, lc])\n",
    "\n",
    "#el número de folds es inválido, incluso no recomendado directamente en la documentación de \n",
    "#Spark, sin embargo, es un punto de inicio. \n",
    "#\n",
    "#Este componente no se *iba* a utilizar en el Magic Loop\n",
    "cv0 = CrossValidato\n",
    "r(estimator=pipeline0,\n",
    "                          estimatorParamMaps=pgRF,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"DEPARTURE_DELAY\"),\n",
    "                          numFolds=3)\n",
    "print(\"Starting fit...\")\n",
    "%time cvM0 = cv0.fit(train)\n",
    "print(\"Starting transform...\")\n",
    "%time preds0 = cvM0.transform(test)\n",
    "preds0.show()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<pre>\n",
    "#pruebas de cómo extraer las etapas del pipeline (en este caso el clasificador)\n",
    "rfFinal = cvM0.bestModel.stages[6]\n",
    "#prueba de extraer las variables del clasificador\n",
    "rfParam = rfFinal.extractParamMap()\n",
    "#obtener el evaluador para medi rerrores\n",
    "ev0 = cvM0.getEvaluator()\n",
    "print(\"Random Forest Parameters: maxDepth=%d, numTrees=%d\"%\n",
    "      (rfParam[rfFinal.maxDepth], rfParam[rfFinal.numTrees]))\n",
    "#aquí probé como se veían los errores\n",
    "print(\"Error %s: %f\"% (ev0.getMetricName(), ev0.evaluate(preds0)))\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "\n",
    "Sin duda el `Magic Loop` es una gran herramienta para iniciar _auto ML_. Sin embargoo, observé que en Machine Learning muchos modelos toman demasiado tiempo, y son prometedores y populares a utilizarse frecuentemente, como RandomForest. Los resultados son variados, por lo que sin importar el número de folds se puede tener una buena idea de cómo se comportan, siendo esto un poco de prueba y error; cosa que no permite auto ML de manera rápida, a menos que se tengan recursos ilimitados (sobretodo de tiempo) y disponibles.\n",
    "\n",
    "Es un poco quisquilloso utilizar Spark al compararlo con el noble R, tengo que aceptarlo. Aún con mi background de Ingeniero en Sistemas. Es claro que yo inicialmente ví con malos ojos R, y quería puro Python. Me es claro que es mucho más escalable y estable (para producción). Pero R es sumamente noble para el aprendizaje, pruebas y demás; comparado con el rigor de Python (y más aún en Java/Scala). Lástima que R no es tan escalable.\n",
    "\n",
    "Además Python no tiene ggplot que tanto nos gusta. Pero combinar las herramientas y utilizarlas sabiamente (esto después de cierto sufrimiento por batallar terminé aprendiendo, apreciando y teniendo no una visión superficial de las cosas, para poder ejercer en el campo.\n",
    "\n",
    "Sin duda seguiré utilizando python, scikit-learn y Spark, pero también es altamente probable que siga utilizando R para hacer pruebas y pequeños modelos, dado que es sumamente rápido realizarlas. Cada herramienta a cada aplicación, aprovechándo sus fortalezas y considerando sus debilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "* <https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42>\n",
    "* <https://spark.apache.org/docs/2.3.0/ml-classification-regression.html#generalized-linear-regression>\n",
    "* <https://spark.apache.org/docs/latest/api/python/index.html>\n",
    "* <https://spark.apache.org/docs/2.2.0/ml-pipeline.html>\n",
    "* <https://stackoverflow.com/questions/37278999/logistic-regression-with-spark-ml-data-frames>\n",
    "* <https://elbauldelprogramador.com/en/how-to-convert-column-to-vectorudt-densevector-spark/>\n",
    "* <https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial>\n",
    "* <https://github.com/apache/spark/blob/master/examples/src/main/python/ml/generalized_linear_regression_example.py>\n",
    "* <https://github.com/apache/spark/blob/master/data/mllib/sample_linear_regression_data.txt>\n",
    "* <https://en.m.wikipedia.org/wiki/F1_score>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
