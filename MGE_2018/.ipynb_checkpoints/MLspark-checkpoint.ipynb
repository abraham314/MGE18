{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init('/home/abraham/spark-2.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# En Spark se puede crear un DataFrame de un RDD, de una lista o de un DataFrame de Pandas, \n",
    "# aquí lo estamos creando con una lista que contiene tuplas de (label, features)\n",
    "# tal cual lo hacíamos en sklearn, y le estamos agregando los nombres de cada columna.\n",
    "# Vectors.dense recibe una lista como parámetro\n",
    "# Este DataFrame lo estamos ocupando como nuestro set de entrenamiento mock!\n",
    "training = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Como lo hacíamos en sklearn, primero instanciamos el modelo que quremos\n",
    "# ocupar con los parámetros que nosotros queremos tener para este \n",
    "# modelo en particular --configuramos el modelo--\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Veamos la documentacion del modelo y que parametros le pusimos a nuestra\n",
    "# configuracion\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ocupemos el modelo que configuramos para entrenar con lo datos que\n",
    "# creamos en el DataFrame training\n",
    "model_1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: \n",
      "{Param(parent='LogisticRegression_4320b18412129384a792', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_4320b18412129384a792', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_4320b18412129384a792', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_4320b18412129384a792', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_4320b18412129384a792', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_4320b18412129384a792', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_4320b18412129384a792', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_4320b18412129384a792', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06, Param(parent='LogisticRegression_4320b18412129384a792', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_4320b18412129384a792', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_4320b18412129384a792', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_4320b18412129384a792', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_4320b18412129384a792', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_4320b18412129384a792', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_1 es un transfomer creado a traves de un estimador (LogisticRegression)\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "# aqui estamos obteniendo la configuracion con la que se entreno\n",
    "# la regresion logistica que ocupamos\n",
    "print(lr.extractParamMap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tambien podemos especificar los parametros con los que queremos que \n",
    "# corra el modelo utilizando el diccionario de ParamMap\n",
    "# Creamos un diccionario -se puede llamar como quieras!- que tenga\n",
    "# como llave el nombre del parametro que quieres modificar, con el valor\n",
    "# correspondiente.\n",
    "param_map = {lr.maxIter: 20}\n",
    "# Si el valor ya existe en el diccionario puedes actualizarlo\n",
    "param_map[lr.maxIter] = 30  \n",
    "# Tambien puedes actualizar varios parametros del diccionario al mismo tiempo\n",
    "param_map.update({lr.regParam: 0.1, lr.threshold: 0.55}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Se pueden combinar diferentes diccionarios...realmente puedes tener\n",
    "# un solo diccionario con los parametros de diversos modelos que ocupes en el\n",
    "# pipeline sin ningun problema, pues el valor asociado es por objeto (ID)\n",
    "# aqui estamos cambiando el nombre de la columna que guarda la salida del\n",
    "# modelo, por default se llama 'probability' -> verificar documentacion del \n",
    "# metodo\n",
    "param_map_2 = {lr.probabilityCol: \"my_probability\"}  \n",
    "param_map_combined = param_map.copy()\n",
    "param_map_combined.update(param_map_2)\n",
    "#puedes ver el contenido del diccionario con param_map_combined.items() -> python 3.5.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fit using parameters: \n",
      "{Param(parent='LogisticRegression_4320b18412129384a792', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_4320b18412129384a792', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_4320b18412129384a792', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_4320b18412129384a792', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_4320b18412129384a792', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_4320b18412129384a792', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_4320b18412129384a792', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_4320b18412129384a792', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06, Param(parent='LogisticRegression_4320b18412129384a792', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_4320b18412129384a792', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_4320b18412129384a792', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_4320b18412129384a792', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_4320b18412129384a792', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_4320b18412129384a792', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Entrenemos una segunda regresion logistica con los nuevos parametros que \n",
    "# establecimos a traves del paramMap\n",
    "# En este fit estamos enviando tanto los datos como los parametros a ocupar en el\n",
    "# modelo de regresion logistica\n",
    "model_2 = lr.fit(training, param_map_combined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "# aqui queremos ver cono que parametros se quedo configurado el modelo\n",
    "# que ocupamos para entrenar\n",
    "print(lr.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creemos el data frame que tendra los datos de prueba mock!\n",
    "test = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "# Ahora si, hagamos predicciones sobre el set de pruebas ocupando el cerebro\n",
    "# antes entrenado utilizando el transform\n",
    "prediction = model_2.transform(test)\n",
    "# la respuesta es un DataFrame (la salida de un transform en su DataFrame)\n",
    "# por lo que podemos aplicarle los metodos de SparkSQL :)\n",
    "# verificamos que si es un DataFrame...\n",
    "type(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# veamos que columnas tiene este DataFrame (como el names de R)\n",
    "prediction.columns\n",
    "# Aqui estamos seleccionando las columnas features, label, \n",
    "# my_probability -> que es el nombre que nosotros especificamos anteriormente en\n",
    "# ParamMap, y la columna prediction que es el nombre por default que regresa\n",
    "# el modelo al parametro 'predictionCol' -> ver documentacion\n",
    "# el collect hara que se regresen los resultados al drive!!! \n",
    "result = prediction.select(\"features\", \"label\", \"my_probability\", \"prediction\") \\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.0570730417103,0.94292695829], prediction=1.0\n",
      "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.92385223117,0.0761477688296], prediction=0.0\n",
      "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.109727761148,0.890272238852], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "for row in result:\n",
    "    print(\"features={}, label={} -> prob={}, prediction={}\".format( \\\n",
    "    row.features, row.label, row.my_probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# al igual que en el ejemplo anterior, creamos un dataframe a traves\n",
    "# de una lista con los datos de entrenamiento, la lista esta formada\n",
    "# por tuplas (id, texto, label). Esta forma no nos servirá para poder meterla\n",
    "# en los objetos de ML, pero mas adelante arreglaremos esto\n",
    "training = sqlContext.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos los transformers: Tokenizer y HashingTF, y los \n",
    "# etimators: LogisticRegression que ocuparemos. Nota que aqui no hemos hecho\n",
    "# ningun fit todavia... la magia vendra mas adelante ;)\n",
    "# Tokenizer convierte el string de entrada (inputCol) a minusculas y separa en\n",
    "# palabras utilizando ocmo separador el espacio\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# HashingTF permite hashear cada palabra utilizando MurmurHash3 convirtiendo\n",
    "# el hash generado en el indice a poner en el \"TDM\". Este metodo optimiza el\n",
    "# tiempo para generar el TDM de TF-IDF \"normal\". Para evitar colisiones en\n",
    "# la conversion a hash se aumenta el numero de bucket -se recomienda ocupar\n",
    "# potencias de 2 para balancear las cubetas-\n",
    "# Nota que en este transformet estamos ocupando como entrada la salida del\n",
    "# transformer Tokenizer\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# Ocuparemos una regresion logistica de nuex\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "# Aqui viene lo bonito... definimos un pipeline que tiene como etapas/pasos\n",
    "# primero el tokenizer, luego el hasing y luego la regresion logistica. Aqui\n",
    "# estamos definiendo el flujo de procesamiento, el DAG! \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voila, solo se requiere de hacer fit al pipeline para que esto funcione\n",
    "# como un pipeline, siguiendo el orden de los pasos establecidos en la \n",
    "# definicion del pipeline :) ... recuerda que el fit solo es como \n",
    "# el entrenamiento una vez que ya definimos las configuraciones de \n",
    "# los objetos que ocuparemos (transformers y estimators)\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos el dataframe de pruebas mock! -> Nota que aqui no hay \n",
    "# label!!!! (asi funcionaria en produccion cierto!)\n",
    "test = sqlContext.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lixto, \"ejecutamos\" el pipeline haciendo un transform al pipeline para \n",
    "# obtener las predicciones del set de pruebas\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.159640773879,0.840359226121], prediction=1.0\n",
      "(5, l m n) --> prob=[0.837832568548,0.162167431452], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.0692663313298,0.93073366867], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.982157533344,0.0178424666556], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "# De nuevo, prediction es un DataFrame generado con un transformer generado\n",
    "# a traves de estimadores y transformers :) \n",
    "# Seleccionamos las columnas que queremos ver \n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"({}, {}) --> prob={}, prediction={}\".format( \\\n",
    "    rid, text, str(prob), prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos nuestro set de entrada para formar la TDM\n",
    "sentence_data = sqlContext.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ocupamos el transformer Tokenizer para separar por palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# Aqui no hay train! porque no estamos entrenando nanda... estamos en un problema\n",
    "# de IR. Tokenizer no tiene un metodo fit -no hay entrenamiento-\n",
    "words_data = tokenizer.transform(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------------+\n",
      "|label|sentence                           |words                                     |raw_features                                         |\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |(16,[0,4,5,9,14],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(16,[0,2,3,7,11,13,15],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(16,[1,6,8,10,12],[1.0,1.0,1.0,1.0,1.0])             |\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ocupamos el transformer CountVectorizer para generar una matriz de \n",
    "# terminos y sus frecuencias \n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "featurized_model = count_vectorizer.fit(words_data)\n",
    "featurized_data = featurized_model.transform(words_data)\n",
    "featurized_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                       |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(16,[0,4,5,9,14],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                            |\n",
      "|0.0  |(16,[0,2,3,7,11,13,15],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
      "|1.0  |(16,[1,6,8,10,12],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                            |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ocupamos IDF para obtener el IDF de la coleccion de documentos mock que \n",
    "# generamos. IDF si tiene un metodo fit a traves del cual le enviamos el set \n",
    "# de tokens al que queremos obtener el IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=1)\n",
    "# Aqui obtenemos el modelo a ocupar (transformer) a ocupar \n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select(\"label\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# creamos nuestro set de datos de entrada categorico\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "# Esta funcion agrega un id numerico a cada valor diferente de un valor categorico \n",
    "# es como establecer los niveles en R de una factor pero los niveles son numericos,\n",
    "# sus id. El indice se establece por orden de frecuencia (descendente), por lo que \n",
    "# el indice 0 corresponde a la variable que aparece con mas frecuencia\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = string_indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    "\n",
    "# OneHotEncoder no tiene un fit ya que solo es un transformador\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n",
      "Features scaled to range: [0.0, 1.0]\n",
      "+--------------+---------------+\n",
      "|features      |scaled_features|\n",
      "+--------------+---------------+\n",
      "|[1.0,0.1,-1.0]|[0.0,0.0,0.0]  |\n",
      "|[2.0,1.1,1.0] |[0.5,0.1,0.5]  |\n",
      "|[3.0,10.1,3.0]|[1.0,1.0,1.0]  |\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data_frame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "data_frame.show()\n",
    "\n",
    "# Configuramos el estimator MinMaxScaler como lo necesitamos\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Creamos el modelo MinMaxScaler (transformer)\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos reescalando \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "# Nota que cuando pedimos getMin y getMax lo hacemos al estimator, no al modelo\n",
    "print(\"Features scaled to range: [{}, {}]\".format(scaler.getMin(), scaler.getMax()))\n",
    "scaled_data.select(\"features\", \"scaled_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "|id |features      |scaled_features                |\n",
      "+---+--------------+-------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[-1.0,-0.6657502859356826,-1.0]|\n",
      "|1  |[2.0,1.1,1.0] |[0.0,-0.4841820261350419,0.0]  |\n",
      "|2  |[3.0,10.1,3.0]|[1.0,1.1499323120707245,1.0]   |\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Creamos el data frame que queremos estandarizar\n",
    "data_frame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "# Configuramos el estimator StandarScaler como lo necesitamos (por default\n",
    "# withMean esta en False porque hace que se regrese un vector dense...\n",
    "# hay que tener cuidado con eso cuando estemos manejandoo vectores sparse\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "# Creamos el modelo StandardScaler para los datos de entrada\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "scaled_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
